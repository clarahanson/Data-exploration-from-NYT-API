from __future__ import division

### NOTE: The nx.read_edgelist() function struggles to read isolated nodes at the beinning of an edge list
### code may be improved by shorting edge list and removing spacing irregularity 

def make_subject_network(input_directory, output_directory, groupby='decade', restrict_vocabulary=False):

    global weighted_edgelist
    # this code creates a bipartite network using articles collected from the New York Time's API
    # as the top level nodes, and the subjects associated with those articles as the bottom level nodes.
    # the network is projected onto the subjects, and the final graph represents the relationship between subjects
    # based on the strength of co-appearance in articles
    #
    # graphs may be produced for all articles in a year or in a decade using the groupby input
    
    import os, re, json, sys, community, operator, csv
    import pandas as pd
    import networkx as nx
    from networkx.algorithms import bipartite
    from collections import Counter
    
    os.chdir(input_directory)
    files = [f for f in os.listdir('.') if f != ".DS_Store"]
    
    # loading and parsing data into {year:list of subject lists} format
    network_data = {}
    for file in files:
        with open(file, 'r') as f:
            r = json.loads(f.read())
            for each in r:
                if groupby=='year':
                    q = re.match('(\d\d\d\d)', each['date'])
                if groupby=='decade':
                    q = re.match('(\d\d\d)', each['date'])
                year = q.group(1)
                each['subjects']=(re.sub('\W', '_', i) for i in each['subjects'])
                if year in network_data:
                    network_data[year].append([i.lower() for i in each['subjects']])
                if year not in network_data:
                    network_data[year]=[[i.lower() for i in each['subjects']]]
    
    # Option to restrict vocabulary of subjects - set here to include only subjects that occur >= 10 times across entire corpus
    if restrict_vocabulary==True:
        total_vocabulary = []
        for value in network_data.values():
            for _list_ in value:
                total_vocabulary.extend(_list_)

        included_words = set(key for (key, value) in Counter(total_vocabulary).items() if value>=10)
        included_words.remove('no_index_terms')

        for key, value in network_data.items():
            completed_list = []
            for each in value:
                completed_list.append([i for i in each if i in included_words])
            network_data[key]=completed_list
                    
    os.chdir(output_directory)
            
    for key, _list_ in network_data.items():
        B = nx.Graph()
        for counter, _sublist_ in enumerate(_list_):
            B.add_node(counter, bipartite=0)
            for subject in _sublist_:
                if B.has_node(subject):
                    B.add_edge(counter, subject)
                else:
                    B.add_node(subject, bipartite=1)
                    B.add_edge(counter, subject)
        top_nodes = set(n for n,d in B.nodes(data=True) if d['bipartite']==0)
        bottom_nodes = set(B) - top_nodes
        # note: the edge weights are a count of the number of shared nodes. Using ratio=True
        #       returns a graph with edges weighted by a ratio of shared edges to possible shared edges
        #       see: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.bipartite.projection.weighted_projected_graph.html#networkx.algorithms.bipartite.projection.weighted_projected_graph
        weighted_edgelist = bipartite.weighted_projected_graph(B, bottom_nodes, ratio=False)
        nx.write_edgelist(weighted_edgelist, key+str(0)+"s_bipartite.txt")
                        
input_directory = " "
output_directory = " "

make_subject_network(input_directory, output_directory, groupby='decade', restrict_vocabulary=True)
